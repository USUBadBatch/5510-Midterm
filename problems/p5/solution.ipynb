{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Control and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) The merits and demerits of reinforcement learning for mechatronic systems\n",
    "\n",
    "Reinforcement learning emphasizes maximizing a reward function by having an agent choose actions in an environment. In the case of robotics, the agent is the robot and the environment is the world in which the robot interacts with.\n",
    "\n",
    "Reinforcement learning can be quite useful for mechatronic systems. Because there are a lot of uncertainties within a mechatronic system, it can be difficult to determine what actions to take to accomplish certain tasks. With reinforcement learning, the system can learn what actions to take to accomplish a given task based on a reward function. This allows the program to manage a lot of the uncertainty that exists within the system. \n",
    "\n",
    "Reinforcement learning involves the interaction of the agent with its environment. Therefore, the agent is choosing its next action based on its interaction within its environment. This can be of great use in a mechatronic system as the mechatronic system interacts with its surrounding environment to accomplish a given task. Applying reinforcement learning to a mechatronic system trains the system to make decisions based on its environment.\n",
    "\n",
    "Reinforcement learning also does not require large amounts of preexisting data. Training a mechatronic system to accomplish a specific task does not come with a large amount of preexisting data that a model can be trained on. A reinforcement learning model learns based on the effect an action has on the observation space.\n",
    "\n",
    "Reinforcement learning can be useful for mechatronic systems; however, the process of obtaining experience on a real system is costly. This requires a model that learns much quicker than a model that may be applied in other contexts. This greatly increases the difficulty to implement a usable reinforcement model for a mechatronic system. You could attempt to create a simulation to model the actions of the mechatronic system, but attempting to replicate a real-world environment within a simulation is nearly impossible for the precision that is sometimes required for a mechatronic system.\n",
    "\n",
    "The true state of environment can never be completely observed and noise-free in a mechatronic system either. Therefore, the reinforcement learning model cannot know precisely in which state it is in. This makes it much more difficult to measure what affect the action performed had on the environment. This increases the complexity of the problem in the mechatronic system because it has to learn from a partially observed environment.\n",
    "\n",
    "Because reinforcement learning for mechatronic systems deals with real-world environments rather than simulated ones, the reward function must account for real-world experience. This greatly increases the difficulty of developing a reward function that quickly learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Reinforcement Learning vs. Controllers\n",
    "\n",
    "![Reinforcement vs. Controller Diagrams](images/controller_vs_reinforcement.png)\n",
    "\n",
    "Reinforcement learning and controllers are similar in that they take information from their environment's state to decide their next action. Controllers, for example, process measured signals according to knowlege about the process and drive the system to approximate a desired behavior. Reinforcement learning models interact with an environment, which provides a reward, and takes actions to maximize that reward.\n",
    "\n",
    "Controllers, however, require knowledge of the environment they are interacting with in order to provide instruction to the actuators to drive a desired behavior. We do not always have precise knowledge of our environment, so controllers are not always practical to use.\n",
    "\n",
    "Reinforcement learning models do not rely on knowledge of the environment. They aim to take actions that maximize a reward function, so they make an action, take the state of the environment following that action, and utilize the reward function to determine what action to take.\n",
    "\n",
    "The main difference between a controller and reinforcement learning model is the knowledge of the environment robot is interacting with. As I stated previously, a controller requires a lot more knowledge of the environment in order to choose meaningful and accurate decisions to accomplish a desired behavior. Introducing too many uncertainties greatly hinders the system's ability to make accurate decisions within the system. Reinforcement learning models rely on the results of their previous action, so they are improving their decision upon each iteration of interaction with the environment. This requires significantly less knowledge of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Reinforcement Learning Model for CartPole\n",
    "\n",
    "The reinforcement model we used is a simple neural network consisting of only two linear layers. It simply takes in the observation space of the state as its training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hamar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   0 | Moving-Average Steps: 9769.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "log_iter = 1\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "agent = Agent(env, load_exist=True)\n",
    "learning = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    steps = 0\n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    cumulative_rewards = 0\n",
    "    action_probs = []\n",
    "\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        \n",
    "        action, action_prob = agent.act(state)\n",
    "        action_probs.append(action_prob)\n",
    "\n",
    "        state, reward, done, *_ = env.step(action)\n",
    "        cumulative_rewards += reward\n",
    "        \n",
    "    policy_loss = torch.cat([-log_prob * cumulative_rewards for log_prob in action_probs]).sum()\n",
    "    \n",
    "    agent.optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    agent.optimizer.step()\n",
    "    \n",
    "    learning.append(steps)\n",
    "    if i % log_iter == 0:\n",
    "        print(f\"Iteration: {i:3d} | Moving-Average Steps: {np.mean(learning[-log_iter:]):.4f}\")\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a graph of the steps the cartpole accomplished before failure. As the model improves over time, there should be an increase in the number of steps the agent is able to perform before failure and the next epoch begins. As you can see, the model greatly increased the number of steps over time until approximately 500 epochs. The model then significantly decreased in its performance.\n",
    "\n",
    "*Note:* This model was loaded from previous trainings, so the 500 epochs is not the total number of epochs the model had trained for.\n",
    "\n",
    "![Model Training Plot](images/training_plot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6d9cfa28a42b4d41470f56da08c5543c73f8bc9dcc92946f6322fb4e7a617b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
